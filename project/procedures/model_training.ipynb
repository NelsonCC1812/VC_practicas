{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# ---\n",
    "import sys; sys.path.append('../')\n",
    "\n",
    "from commons.imgs_mean_std import *\n",
    "\n",
    "from commons.dataset import *\n",
    "from commons.imageutils import *\n",
    "from commons.HistCollection import *\n",
    "\n",
    "from modules.img_transforms import *\n",
    "from modules.train_functions import *\n",
    "from modules.Comparator import *\n",
    "\n",
    "from modules.Dataset import *\n",
    "from modules.EarlyStopper import *\n",
    "\n",
    "from modules.Model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants & hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=None\n",
    "\n",
    "IMAGE_SIZE=220\n",
    "\n",
    "BATCH_SIZE=32\n",
    "NUM_EPOCHS=100\n",
    "\n",
    "LEARNING_RATE=.001\n",
    "MOMENTUM=.95\n",
    "\n",
    "EARLY_PATIENCE=20\n",
    "SCHEDULER_PATIENCE=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Devices:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(os.path.join(DATASETS_PATHS.norm_faces.info, 'train_dataset.csv'))\n",
    "val_dataset = pd.read_csv(os.path.join(DATASETS_PATHS.norm_faces.info, 'val_dataset.csv'))\n",
    "test_dataset = pd.read_csv(os.path.join(DATASETS_PATHS.norm_faces.info, 'test_dataset.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization= normalize(IMAGE_SIZE, MEAN, STD)\n",
    "\n",
    "train_dataset = CD_TrippletsCreator(train_dataset, transform=normalization, data_augmentation_tranforms=[data_augmentation()])\n",
    "val_dataset = CD_TrippletsCreator(val_dataset, transform=normalization)\n",
    "test_dataset = CD_TrippletsCreator(test_dataset, transform=normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_callback(**kwarks):\n",
    "    print(\" > Early Stop <\")\n",
    "\n",
    "early_stopper = EarlyStopper(EARLY_PATIENCE, .001, callback=early_callback, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(256, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=4608, out_features=2048, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=512, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelso\\miniconda3\\envs\\vc_project\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.TripletMarginWithDistanceLoss(distance_function=dst).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=.00001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=.1, patience=SCHEDULER_PATIENCE, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 =================================\n",
      "\ttrain_loss: 1.013602\tval_loss: 0.752264\n",
      "\tdst_mean_train (pos, neg): (3.364281, 3.363387) dst_mean_val (pos, neg): (0.789065, 1.060162)\n",
      "Best model currently!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [01:48<05:15, 26.30s/it]"
     ]
    }
   ],
   "source": [
    "def callback(**kwargs):\n",
    "\tprint(f\"Epoch: {kwargs['epoch']} =================================\")\n",
    "\tprint(f\"\\ttrain_loss: {kwargs['train_loss']:2f}\\tval_loss: {kwargs['val_loss']:2f}\")\n",
    "\tprint(f\"\\tdst_mean_train (pos, neg): ({np.mean(kwargs['train_h'].posit_dst[-1]):2f}, {np.mean(kwargs['train_h'].negat_dst[-1]):2f}) dst_mean_val (pos, neg): ({np.mean(kwargs['val_h'].posit_dst[-1]):2f}, {np.mean(kwargs['val_h'].negat_dst[-1]):2f})\")\n",
    "\n",
    "\n",
    "train_h, val_h = train_loop(model, train_loader, val_loader=val_loader,\n",
    "                            optimizer=optimizer, criterion=criterion, dst=dst, num_epoch=100, device=device,\n",
    "                            early_stopper=early_stopper, scheduler=scheduler, callback=callback)\n",
    "\n",
    "model.load_state_dict(early_stopper.best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'model.pth', complete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_analisys(**kwargs):\n",
    "\n",
    "\ttrain_posit_mean_h = [np.mean(elm) for elm in  kwargs['train_h'].posit_dst]\n",
    "\ttrain_negat_mean_h = [np.mean(elm) for elm in  kwargs['train_h'].negat_dst]\n",
    "\n",
    "\ttrain_posit_median_h = [np.median(elm) for elm in kwargs['train_h'].posit_dst]\n",
    "\ttrain_negat_median_h = [np.median(elm) for elm in kwargs['train_h'].negat_dst]\n",
    "\n",
    "\tval_posit_mean_h = [np.mean(elm) for elm in  kwargs['val_h'].posit_dst]\n",
    "\tval_negat_mean_h = [np.mean(elm) for elm in  kwargs['val_h'].negat_dst]\n",
    "\n",
    "\tval_posit_median_h = [np.median(elm) for elm in kwargs['val_h'].posit_dst]\n",
    "\tval_negat_median_h = [np.median(elm) for elm in kwargs['val_h'].negat_dst]\n",
    "\n",
    "\n",
    "\tplt.figure(figsize=(24, 6))\n",
    "\n",
    "\tplt.subplot(1, 3, 1); plt.title('Train / Val loss')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.xticks([1, len(kwargs['val_h'].loss)])\n",
    "\tplt.plot(kwargs['train_h'].loss, label='Train')\n",
    "\tplt.plot(kwargs['val_h'].loss, label='Val')\n",
    "\tplt.plot(kwargs['best_loss'][0]-1, kwargs['best_loss'][1], 'o', )\n",
    "\tplt.annotate('best', (kwargs['best_loss'][0]-1, kwargs['best_loss'][1]))\n",
    "\tplt.axvline(kwargs['best_loss'][0]-1, linestyle='dashed', color='red', linewidth=1)\n",
    "\tplt.legend()\n",
    "\n",
    "\tplt.subplot(1, 3, 2); plt.title('Distances means')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel('Mean')\n",
    "\tplt.xticks([1, len(kwargs['val_h'].loss)])\n",
    "\tplt.plot(train_posit_mean_h, label='Train positive')\n",
    "\tplt.plot(train_negat_mean_h, label='Train negative')\n",
    "\tplt.plot(val_posit_mean_h, label='Val positive')\n",
    "\tplt.plot(val_negat_mean_h, label='Val negative')\n",
    "\tplt.axvline(kwargs['best_loss'][0]-1, linestyle='dashed', color='red', linewidth=1)\n",
    "\tplt.legend()\n",
    "\n",
    "\tplt.subplot(1,3,3); plt.title('Distances medians')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Median')\n",
    "\tplt.xticks([1, len(kwargs['val_h'].loss)])\n",
    "\tplt.plot(train_posit_median_h, label='Train positive')\n",
    "\tplt.plot(train_negat_median_h, label='Train negative')\n",
    "\tplt.plot(val_posit_median_h, label='Val positive')\n",
    "\tplt.plot(val_negat_median_h, label='Val negative')\n",
    "\tplt.axvline(kwargs['best_loss'][0]-1, linestyle='dashed', color='red', linewidth=1)\n",
    "\tplt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_analisys(train_h=train_h, val_h=val_h, best_loss=(early_stopper.best_epoch, early_stopper.best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_h = eval(model, test_loader, device=device, criterion=criterion, dst=dst)\n",
    "print('Loss', test_h.loss)\n",
    "print(f'Posit dst:\\t| mean {np.mean(test_h.posit_dst)}\\t| median {np.median(test_h.posit_dst)}\\t| std {np.std(test_h.posit_dst)}\\t| min, max {np.min(test_h.posit_dst)}, {np.max(test_h.posit_dst)}\\t| diff(min,max) {np.max(test_h.posit_dst)-np.min(test_h.posit_dst)}')\n",
    "print(f'Negat dst:\\t| mean {np.mean(test_h.negat_dst)}\\t| median {np.median(test_h.negat_dst)}\\t| std {np.std(test_h.negat_dst)}\\t| min, max {np.min(test_h.negat_dst)}, {np.max(test_h.negat_dst)}\\t| diff(min,max) {np.max(test_h.negat_dst)-np.min(test_h.negat_dst)}')\n",
    "print(f'Diff(max(posit_dst), min(negat_dst)) {min(test_h.negat_dst) - max(test_h.posit_dst)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segundo entrenamiento con el mejor modelo actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_h_, val_h_ =  train_h.copy(), val_h.copy()\n",
    "train_h_.loss, val_h_.loss = train_h_.loss[:early_stopper.best_epoch], val_h_.loss[:early_stopper.best_epoch]\n",
    "train_h_.posit_dst, val_h_.posit_dst  = train_h_.posit_dst[:early_stopper.best_epoch], val_h_.posit_dst[:early_stopper.best_epoch]\n",
    "train_h_.negat_dst, val_h_.negat_dst  = train_h_.negat_dst[:early_stopper.best_epoch], val_h_.negat_dst[:early_stopper.best_epoch]\n",
    "train_h_.asetid, val_h_.asetid  = train_h_.asetid[:early_stopper.best_epoch], val_h_.asetid[:early_stopper.best_epoch]\n",
    "train_h_.nsetid, val_h_.nsetid = train_h_.nsetid[:early_stopper.best_epoch], val_h_.nsetid[:early_stopper.best_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper.reset()\n",
    "\n",
    "train_h_, val_h_ = train_loop(model, train_loader, val_loader=val_loader, initial_epoch=early_stopper.best_epoch,\n",
    "                            optimizer=optimizer, criterion=criterion, dst=dst, num_epoch=100, device=device,\n",
    "                            early_stopper=early_stopper, scheduler=scheduler, callback=callback, train_h=train_h_, val_h=val_h_)\n",
    "\n",
    "model.load_state_dict(early_stopper.best_model)\n",
    "save_model(model, 'model.pth', complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(early_stopper.best_model)\n",
    "save_model(model, 'model.pth', complete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisys again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_analisys(train_h=train_h_, val_h=val_h_, best_loss=(early_stopper.best_epoch, early_stopper.best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_h = eval(model, test_loader, device=device, criterion=criterion, dst=dst)\n",
    "print('Loss', test_h.loss)\n",
    "print(f'Posit dst:\\t| mean {np.mean(test_h.posit_dst)}\\t| median {np.median(test_h.posit_dst)}\\t| std {np.std(test_h.posit_dst)}\\t| min, max {np.min(test_h.posit_dst)}, {np.max(test_h.posit_dst)}\\t| diff(min,max) {np.max(test_h.posit_dst)-np.min(test_h.posit_dst)}')\n",
    "print(f'Negat dst:\\t| mean {np.mean(test_h.negat_dst)}\\t| median {np.median(test_h.negat_dst)}\\t| std {np.std(test_h.negat_dst)}\\t| min, max {np.min(test_h.negat_dst)}, {np.max(test_h.negat_dst)}\\t| diff(min,max) {np.max(test_h.negat_dst)-np.min(test_h.negat_dst)}')\n",
    "print(f'Diff(max(posit_dst), min(negat_dst)) {min(test_h.negat_dst) - max(test_h.posit_dst)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
