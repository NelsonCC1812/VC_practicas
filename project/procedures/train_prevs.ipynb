{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---\n",
    "import sys; sys.path.append('../')\n",
    "from commons.dataset import *\n",
    "import modules.img_normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_INPUT = DATASETS_PATHS.faces\n",
    "DATASET_OUTPUT = DATASETS_PATHS.norm_faces\n",
    "\n",
    "IMAGE_SIZE = 220\n",
    "\n",
    "TRAIN_PERCENT = .65# .7\n",
    "VAL_PERCENT = .2\n",
    "TEST_PERCENT = .15# .1\n",
    "RANDOM_STATE = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(DATASET_INPUT.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m     pixels \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     16\u001b[0m mean \u001b[38;5;241m=\u001b[39m suma\u001b[38;5;241m/\u001b[39mpixels\n\u001b[1;32m---> 17\u001b[0m std \u001b[38;5;241m=\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msquared_sum\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mpixels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> Media \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> Desviación típica \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "# Mean & std\n",
    "\n",
    "suma = 0.0\n",
    "squared_sum = 0.0\n",
    "count = 0\n",
    "pixels = 0\n",
    "\n",
    "for path in dataset.path.iloc:\n",
    "    img = cv2.imread(path)\n",
    "    suma += img.sum()\n",
    "    squared_sum += (img**2).sum()\n",
    "    count += 1\n",
    "    pixels += img.shape[0]*img.shape[1]\n",
    "\n",
    "\n",
    "mean = suma/pixels\n",
    "std = math.sqrt((squared_sum/pixels) -( mean**2))\n",
    "\n",
    "print(f'> Media {mean:.3}')\n",
    "print(f'> Desviación típica {std:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for idx,path in enumerate(dataset.path.sample(3).iloc):\n",
    "    plt.subplot(1,3,idx+1)\n",
    "    plt.imshow(normalizer(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_faces_dataset = pd.DataFrame(columns=dataset.columns)\n",
    "\n",
    "count = 0\n",
    "def process(entry):\n",
    "    cv2.imwrite(os.path.join(DATASET_OUTPUT.data, f'{count:3}.png'),\n",
    "        cv2.cvtColor(normalizer(\n",
    "            cv2.cvtColor(cv2.imread(entry.path), cv2.COLOR_BGR2RGB)), cv2.COLOR_RGB2BGR))\n",
    "    count+=1\n",
    "\n",
    "dataset.apply(process, axis=1)\n",
    "norm_faces_dataset.to_csv(DATASET_OUTPUT.info, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data slicing in training, validation and tests groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tsetid\tmin-max\tproportion\n",
      "> train: \t287  \t9-10 \t0.65977\n",
      "> val: \t\t87  \t3-3 \t0.20000\n",
      "> test: \t61  \t2-3 \t0.14023\n"
     ]
    }
   ],
   "source": [
    "train_dataset, tmp = train_test_split(dataset, train_size=TRAIN_PERCENT, stratify=dataset.setid, shuffle=True, random_state=RANDOM_STATE)\n",
    "val_dataset, test_dataset = train_test_split(tmp, test_size=TEST_PERCENT/(TEST_PERCENT+VAL_PERCENT), stratify=tmp.setid, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print('\\t\\tsetid\\tmin-max\\tproportion')\n",
    "print(f\"> train: \\t{train_dataset.count(axis=1).size}  \\t{train_dataset.setid.value_counts().min()}-{train_dataset.setid.value_counts().max()} \\t{train_dataset.count(axis=1).size/dataset.count(axis=1).size:.5f}\")\n",
    "print(f\"> val: \\t\\t{val_dataset.count(axis=1).size}  \\t{val_dataset.setid.value_counts().min()}-{val_dataset.setid.value_counts().max()} \\t{val_dataset.count(axis=1).size/dataset.count(axis=1).size:.5f}\")\n",
    "print(f\"> test: \\t{test_dataset.count(axis=1).size}  \\t{test_dataset.setid.value_counts().min()}-{test_dataset.setid.value_counts().max()} \\t{test_dataset.count(axis=1).size/dataset.count(axis=1).size:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path\n",
      "False    287\n",
      "Name: count, dtype: int64\n",
      "\n",
      "path\n",
      "False    287\n",
      "Name: count, dtype: int64\n",
      "\n",
      "path\n",
      "False    87\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((train_dataset.path.isin(test_dataset.path) == True).value_counts(), end='\\n\\n')\n",
    "print((train_dataset.path.isin(val_dataset.path) == True).value_counts(), end='\\n\\n')\n",
    "print((val_dataset.path.isin(test_dataset.path) == True).value_counts(), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / load datasets slices (train, val, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_csv(os.path.join(DATASETS_PATHS.norm_faces.info, 'train_dataset.csv'), index=False)\n",
    "val_dataset.to_csv(os.path.join(DATASETS_PATHS.norm_faces.info, 'val_dataset.csv'), index=False)\n",
    "test_dataset.to_csv(os.path.join(DATASETS_PATHS.norm_faces.info, 'test_dataset.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(os.path.join(DATASETS_PATHS.norm_faces.info, 'train_dataset.csv'))\n",
    "val_dataset = pd.read_csv(os.path.join(DATASETS_PATHS.norm_faces.info, 'val_dataset.csv'))\n",
    "test_dataset = pd.read_csv(os.path.join(DATASETS_PATHS.norm_faces.info, 'test_dataset.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
